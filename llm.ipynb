{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Panda-22/LLM-Team2/blob/main/llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install accelerate -U\n",
        "!pip install transformers[torch]"
      ],
      "metadata": {
        "id": "ZdbgT1Q9ZLhh"
      },
      "id": "ZdbgT1Q9ZLhh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8237a6f5",
      "metadata": {
        "scrolled": false,
        "id": "8237a6f5"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import GPT2Tokenizer\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, tokenizer, recipes, block_size):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.samples = []\n",
        "\n",
        "        for recipe in recipes:\n",
        "            text = recipe['title'] + \" \"  # Start with the title\n",
        "            text += \"Ingredients: \" + ', '.join(recipe['ingredients']) + \". \"  # Add ingredients\n",
        "            text += \"Directions: \" + ' '.join(recipe['directions']) + \". \"  # Add directions\n",
        "\n",
        "            # call __call__ from tokenizer for automatic padding\n",
        "            tokenized_text = tokenizer(text, truncation=True, max_length=block_size, padding=\"max_length\", return_tensors=\"pt\")\n",
        "\n",
        "            # adding tokenized_text to samples\n",
        "            self.samples.append(tokenized_text)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # return a dict: input_ids, attention_mask and labels\n",
        "        sample = self.samples[idx]\n",
        "        # for language models, labels equal to input_ids in general\n",
        "        sample[\"labels\"] = sample[\"input_ids\"].clone()\n",
        "        return {key: value.squeeze(0) for key, value in sample.items()}\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
        "\n",
        "# Load dataset\n",
        "# Assuming `dataset` is a list of dictionaries with the given structure\n",
        "# This part depends on how your data is actually loaded\n",
        "dataset = load_dataset('brianarbuckle/cocktail_recipes', split='train')\n",
        "\n",
        "# Prepare the list of recipe texts\n",
        "# Assuming `dataset` yields dictionaries directly\n",
        "recipes = [example for example in dataset]\n",
        "\n",
        "# setting pad_token as eos_token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# recreate dataset\n",
        "my_dataset = MyDataset(tokenizer, recipes, block_size=128)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15d33255",
      "metadata": {
        "id": "15d33255",
        "outputId": "d9880df2-4aae-432a-965d-f33d660ce334"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E:\\python\\envs\\llm\\lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='876' max='876' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [876/876 1:52:12, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>3.295000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.117700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.967300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.915900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.765400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.755700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.654500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.570700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'generated_text': \"Example prompt: When I am not at work, do you think of the night before dinner or the first night I was in this house? It should be an optional choice, but sometimes this simple thing—before dinner—pays huge. Even though many are not traditional dinner-time pousse-café drinks, combining them with a good old-fashioned iced coffee can be the key to a complete night's rest., 2 ounces espresso, 1/2 ounce peach schnapps,\"}]\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel, Trainer, TrainingArguments\n",
        "\n",
        "# loading pretrained model\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
        "\n",
        "# Defining training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    overwrite_output_dir=True,       # overwrite to output directory\n",
        "    num_train_epochs=4,              # number of training epochs\n",
        "    per_device_train_batch_size=4,   # batch size\n",
        "    save_steps=1000,                 # save model per steps\n",
        "    save_total_limit=2,              # total number of saved models\n",
        "    logging_dir='./logs',            # log directory\n",
        "    logging_steps=100,               # save log per steps\n",
        "    prediction_loss_only=True,       # predict the loss only\n",
        "    learning_rate=5e-5,              # learning rate\n",
        "    warmup_steps=500,                # warmup steps\n",
        ")\n",
        "\n",
        "# Initializing trainer\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    train_dataset = my_dataset,  # to use specified dataset\n",
        "    # eval_dataset=eval_dataset,  # to designate evaluation dataset if needed\n",
        ")\n",
        "\n",
        "# start training\n",
        "trainer.train()\n",
        "\n",
        "# to save the fine-tuned model\n",
        "model.save_pretrained('./fine_tuned_model')\n",
        "\n",
        "# to save the tokenizer to the same folder\n",
        "tokenizer.save_pretrained('./fine_tuned_model')\n",
        "\n",
        "# loading fine-tuned model and tokenizer\n",
        "model = GPT2LMHeadModel.from_pretrained('./fine_tuned_model')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('./fine_tuned_model')\n",
        "\n",
        "# creating pipeline\n",
        "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
        "\n",
        "# to generate text\n",
        "print(generator(\"Example prompt\", max_length=100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe9cf9e6",
      "metadata": {
        "id": "fe9cf9e6",
        "outputId": "7ad71806-785c-48bd-a23d-cdfa671fd22c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'generated_text': 'whiskey spritz with amaretto. (iStock) A cocktail from London, named for its famous apothecary, from the 14th century, and originally made frothy with absinthe.. Directions: Shake ingredients with ice. Strain into a chilled cocktail glass.. '}]\n"
          ]
        }
      ],
      "source": [
        "generator = pipeline('text-generation', model='./fine_tuned_model')\n",
        "set_seed(41)\n",
        "\n",
        "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
        "\n",
        "print(generator(\"whiskey\", max_length=100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05b2335d",
      "metadata": {
        "id": "05b2335d",
        "outputId": "6e7e061d-8cf8-4b1c-e7fc-54d1975bfc43"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'generated_text': 'Final Ward Ingredients: 1 Part Ginger Liqueur, 3 Parts Pineapple Juice, 2 Parts GrapeFruit Juice. Directions: shake on ice and strain Garnish with twist of Lemon peel. '}]\n"
          ]
        }
      ],
      "source": [
        "generator = pipeline('text-generation', model='./fine_tuned_model')\n",
        "set_seed(41)\n",
        "\n",
        "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
        "\n",
        "print(generator(\"Final Ward\", max_length=100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "500ea9f9",
      "metadata": {
        "id": "500ea9f9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "LLM",
      "language": "python",
      "name": "llm"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}