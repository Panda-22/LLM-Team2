# -*- coding: utf-8 -*-
"""team2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s6irukgkj-iMljuJEO1jghqqxIKUjGQM

# Large Language Models
## 1. Data preparation
"""

import transformers
print(transformers.__version__)

"""## 2. Parameter setting"""

!pip show tensorflow

"""## Test-GPT-2"""

"""
PyTorch version
"""

from transformers import GPT2Tokenizer, GPT2Model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2Model.from_pretrained('gpt2')
text = "anything here."
encoded_input = tokenizer(text, return_tensors='pt')
output = model(**encoded_input)
print(type(output))
print(encoded_input)
print('wowwwwwwwwwww')

"""## Test: GPT2_API"""

import requests

INPUT_TXT = "Are you serious?"

API_TOKEN = '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'
API_URL = "https://api-inference.huggingface.co/models/openai-community/gpt2"
headers = {"Authorization": f"Bearer {API_TOKEN}"}

def query(payload):
	response = requests.post(API_URL, headers=headers, json=payload)
	return response.json()

output = query({"inputs": INPUT_TXT,})[0]
output['generated_text']

# batch generation

from transformers import pipeline, set_seed
generator = pipeline('text-generation', model='gpt2')
set_seed(42)
generator("Hello, I'm a language model,", max_length=30, num_return_sequences=5)

"""## Test: Gemma-2B"""

from transformers import AutoTokenizer, AutoModelForCausalLM
from huggingface_hub import login
login()
#input: API_TOKEN = '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'

tokenizer = AutoTokenizer.from_pretrained("google/gemma-7b")
model = AutoModelForCausalLM.from_pretrained("google/gemma-7b")

#input_text = "Write me a poem about Machine Learning."
input_text = 'Who is better, you or ChatGPT?'
input_ids = tokenizer(input_text, return_tensors="pt")

outputs = model.generate(**input_ids)
print(tokenizer.decode(outputs[0]))

"""## Test-LLaMA2"""

# Use a pipeline as a high-level helper
from transformers import pipeline

pipe = pipeline("text-generation", model="meta-llama/Llama-2-7b-chat-hf")

# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf")
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-chat-hf")

"""# Train a simple GPT2 model via CoLab - TensorFlow version

##### reference:
 https://batulaiko.medium.com/training-a-gpt-2-model-via-colab-b4c30f1d9adb
"""

# Commented out IPython magic to ensure Python compatibility.
# activate tensorflow
# %tensorflow_version 2.x
import tensorflow as tf

# downloading gpt-2 library from pip
!pip install gpt-2-simple

"""## Choosing GPT-2 Model"""

import gpt_2_simple as gpt2
gpt2.download_gpt2(model_name='124M')
#If you download to a specific location
#gpt2.download_gpt2(model_name='124M',model_dir="/mydrive/models")

"""## Setting Google Drive location"""

# Set drive location as /mydrive
!ln -s /content/drive/MyDrive/ /mydrive
# See inside of /mdrive folder
!ls /mydrive

from google.colab import files
gpt2.mount_gdrive()

"""## Run the gpt-2â€™s finetune for training."""

import tensorflow as tf

tf.compat.v1.reset_default_graph() # reset tensorflow to avoid error "Variable model/wpe already exists, disallowed."
sess = gpt2.start_tf_sess()
gpt2.finetune(sess, dataset="/mydrive/dune_1.txt", steps=200, model_name='124M',run_name="dune",
sample_every=100, save_every=100, print_every=10, restore_from='fresh')

"""## Saving checkpoint and copy to drive"""

gpt2.copy_checkpoint_to_gdrive(run_name = run_name)
gpt2.copy_file_to_gdrive()

#single_text = gpt2.generate(sess, return_as_list = True)
#print(len(single_text))
gpt2.generate(sess, run_name='run1', length=3000) #Length is the number of characters in text.

"""# Train GPT2 model via CoLab - PyTorch Version"""

! pip install expanda

! git clone https://github.com/affjljoo3581/Expanda.git
! cd Expanda
! python setup.py install

"""## GPT2 Interactive Notebook"""

import warnings
warnings.filterwarnings('ignore')

import IPython, torch
IPython.display.HTML(f'Current GPU: {torch.cuda.get_device_name()}')

!rm -rf GPT2
!git clone --quiet https://github.com/affjljoo3581/GPT2

#@title Download resources from Google Cloud Storage

model = 'gs://my-bucket/my-model' #@param {type:"string"}
vocab = 'gs://my-bucket/my-vocab' #@param {type:"string"}
eval_corpus = 'gs://my-bucket/my-eval-corpus' #@param {type:"string"}

!gcloud auth login
!gsutil -q cp vocab vocab.txt
!gsutil -q cp $eval_corpus corpus.txt

#@title Model Configuration
seq_len = 128 #@param {type:"integer"}
layers = 24 #@param {type:"integer"}
heads = 16 #@param {type:"integer"}
dims = 1024 #@param {type:"integer"}
rate = 4 #@param {type:"integer"}

#@title Generation Options
nucleus_prob = 0.8 #@param {type:"slider", min:0, max:1, step:0.01}

import IPython
display(IPython.display.HTML(''''''))

!export PYTHONPATH=GPT2/src; python -m gpt2 generate \
        --vocab_path    vocab.txt \
        --model_path    model.pth \
        --seq_len       layers \
        --heads         dims \
        --rate          nucleus_prob \
        --use_gpu

import IPython
lines = !wc -l corpus.txt | awk '{print $1}'
IPython.display.HTML(f'Total Evaluation Sequences: {lines[0]}')

# Commented out IPython magic to ensure Python compatibility.
# #@title Evaluation Options
# %%time
# batch_eval = 256 #@param {type: "integer"}
# total_steps = 100 #@param {type: "integer"}
# 
# !export PYTHONPATH=GPT2/src; python -m gpt2 evaluate \
#         --model_path    model.pth \
#         --eval_corpus   corpus.txt \
#         --vocab_path    vocab.txt \
#         --seq_len       layers \
#         --heads         dims \
#         --rate          batch_eval \
#         --total_steps   $total_steps \
#         --use_gpu
#